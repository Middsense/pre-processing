{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rayleigh_KNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6xMB_oOKh_c",
        "colab_type": "text"
      },
      "source": [
        "# Rayleigh distributions\n",
        "\n",
        "General ideas: since the original images had real and complex components (where we assume a Gaussian distribution) , we expect the amplitude ($\\sqrt{real^2 + im^2}$) to be Rayleigh distributed and the intensity ($\\text{amplitude}^2$) to be exponentially distributed.\n",
        "\n",
        "The goal here is to fit a Rayleigh distribution to each set of amplitude values for each road, and compare the location and scale parameters to see if different IRI classes have differenlty-shaped distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs1cGPNxLB87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import geopandas\n",
        "\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import rayleigh\n",
        "\n",
        "from glob import glob "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kggbo90x-W-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive for file access - skip this cell if not using Colab\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY-E1pW4KZtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to load datasets into a dictionary\n",
        "\n",
        "def load_data_dict(dir):\n",
        "    \"\"\"\n",
        "    in: path to directory with merged/cleaned .pkl datasets\n",
        "    out: dictionary with all the merged SAR and IRI datasets as dataframes\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    for path in glob(dir + '*'):\n",
        "        key = path.split('/')[-1][:-4]\n",
        "        df = pd.read_pickle(path)\n",
        "        datasets[key] = df\n",
        "    \n",
        "    return datasets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGZ5XPuN7rdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define data directory \n",
        "DATA_DIR = '/content/drive/Shared drives/Remote sensing/Summer 2020/DATA/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l0m3HNWWzbd",
        "colab_type": "text"
      },
      "source": [
        "## Estimate distribution on each road segment\n",
        "\n",
        "The goal here is to calculate the distribution on the pixel data for each OID. Running something like `df.groupby('oid').agg(lambda x: rayleigh.fit(x))` takes a very _very_ long time and is definitely not the way to do it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSW-fdh305uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# road-level data\n",
        "roadlevel_m = load_data_dict(DATA_DIR +'road_level_merged/')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjYUXioGTAXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a pixel-level dataframe \n",
        "pixels = pd.read_pickle(DATA_DIR + 'pixel_level/raw_pixels.pkl')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXJUtckxO6mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Aggregating on one image, since across all images is far too slow \n",
        "\"\"\"\n",
        "data = pixels.loc[:, ['oid_2012_buffered_masked', '20110829', '20120416']]\n",
        "data.columns = ['oid', '20110829', '20120416']\n",
        "\n",
        "def rayfit(g):\n",
        "  # Rayleigh fitting\n",
        "  loc, scale = rayleigh.fit(g)\n",
        "  return loc, scale\n",
        "\n",
        "ray = data.groupby('oid', as_index=True).agg(rayfit)\n",
        "\n",
        "# extract location and scale - this works for several image rows\n",
        "loc = ray.apply(lambda x: x.str[0], axis=1).add_suffix('_loc')\n",
        "scale = ray.apply(lambda x: x.str[1], axis=1).add_suffix('_scale')\n",
        "\n",
        "merged = pd.concat([loc, scale], axis=1, join='outer')\n",
        "# merged.columns = ['loc', 'scale']\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO0jwXomPpKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge above information with existing road-level data \n",
        "df = roadlevel_m['despeck_buffered_masked']\n",
        "merged = pd.concat([df, merged], axis=1, join='outer')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFTq1CnI9v3A",
        "colab_type": "text"
      },
      "source": [
        "Here we just plot the location and scale parameters from a single image. If we had a DataFrame with a _loc and _scale column for every image, we could plot the parameters from the closest SAR acquisition to the IRI test date (same logic as closest_mean and closest_std) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMxHBnbvYUtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot location and scale for one image \n",
        "sns.scatterplot(data=merged, x='20120416_loc', y='20120416_scale', hue='quality')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9TB6g3IQsn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separating by quality\n",
        "g = sns.FacetGrid(data=merged, col='quality', hue='quality')\n",
        "g.map(sns.scatterplot, '20120416_loc', '20120416_scale')\n",
        "plt.xlim(-0.5, 0.5)\n",
        "plt.ylim(0, 2.5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}